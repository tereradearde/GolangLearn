version: "3.9"

services:
  rabbitmq:
    image: rabbitmq:3.13-management
    ports:
      - "5672:5672"
      - "15672:15672"
    healthcheck:
      test: ["CMD-SHELL", "rabbitmq-diagnostics -q ping"]
      interval: 5s
      timeout: 3s
      retries: 10

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 10
  db:
    image: postgres:16-alpine
    environment:
      - POSTGRES_DB=learngo
      - POSTGRES_USER=learngo
      - POSTGRES_PASSWORD=learngo
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U learngo"]
      interval: 5s
      timeout: 3s
      retries: 5
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    environment:
      - APP_ENV=dev
      - HTTP_PORT=:8080
      - DB_DSN=postgres://learngo:learngo@db:5432/learngo?sslmode=disable
      - CORS_ORIGINS=http://localhost:3000
      - JWT_SECRET=dev-secret-change-in-production
      - JWT_TTL_MIN=60
      - JWT_REFRESH_SECRET=dev-refresh-secret-change-in-production
      - JWT_REFRESH_TTL_DAYS=7
      - SEED_DEMO=true
      - ADMIN_EMAIL=admin@example.com
      - ADMIN_PASSWORD=secret123
      - S3_ENDPOINT=s3.twcstorage.ru
      - S3_BUCKET=f2be7c92-c5747428-39c4-4b70-9a69-86da4e1ef2a9
      - S3_ACCESS_KEY=7T3N4701AAAUL34RMAPE
      - S3_SECRET_KEY=Q3BNairajihDCRk4uwWVK8rm11PldtAIEcJKbfou
      - S3_BASE_URL=https://s3.twcstorage.ru
      # OpenAI / AI Configuration
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_MODEL=gpt-4o
      - OPENAI_MAX_TOKENS=500
      - OPENAI_TEMPERATURE=0.7
      - OPENAI_BASE_URL=https://api.openai.com/v1
      # Judge0 / Code Execution Configuration
      - JUDGE0_API_URL=${JUDGE0_API_URL:-https://judge0.com/api/v1}
      - JUDGE0_API_KEY=${JUDGE0_API_KEY:-}
      - CODE_EXECUTION_TIMEOUT=5000
      - CODE_EXECUTION_MEMORY_LIMIT=128
      # Rate Limiting Configuration
      - RATE_LIMIT_AI=60
      - RATE_LIMIT_EXECUTE=100
      - RATE_LIMIT_GLOBAL=1000
    ports:
      - "8080:8080"
    depends_on:
      db:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped

  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./ops/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    ports:
      - "9090:9090"
    depends_on:
      - api

  grafana:
    image: grafana/grafana:10.4.3
    ports:
      - "3001:3001"
    environment:
      - GF_SERVER_HTTP_PORT=3001
    depends_on:
      - prometheus

  runner:
    build:
      context: .
      dockerfile: Dockerfile.runner
    environment:
      - RABBIT_URL=amqp://guest:guest@rabbitmq:5672/
      - RUNNER_QUEUE=runner.jobs
      - REDIS_ADDR=redis:6379
    deploy:
      resources:
        limits:
          cpus: '0.50'
          memory: 256M
    depends_on:
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped


volumes:
  pgdata:


